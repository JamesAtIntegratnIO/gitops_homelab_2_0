alertmanager:
  enabled: true
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: config-nfs-client
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

prometheus:
  service:
    type: ClusterIP
  prometheusSpec:
    enableRemoteWriteReceiver: true
    retention: 15d
    # Disable the behavior that makes {} selector use Helm values
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    # Discover ServiceMonitors across all namespaces
    serviceMonitorNamespaceSelector: {}
    serviceMonitorSelector: {}
    # Discover PodMonitors across all namespaces
    podMonitorNamespaceSelector: {}
    podMonitorSelector: {}
    # Discover PrometheusRules across all namespaces
    ruleNamespaceSelector: {}
    ruleSelector: {}
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: config-nfs-client
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 30Gi

# Configure ServiceMonitors for Talos control plane components
kubeControllerManager:
  enabled: true
  endpoints:
    - 10.0.4.101
    - 10.0.4.102
    - 10.0.4.103
  service:
    enabled: true
    port: 10257
    targetPort: 10257
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true

kubeScheduler:
  enabled: true
  endpoints:
    - 10.0.4.101
    - 10.0.4.102
    - 10.0.4.103
  service:
    enabled: true
    port: 10259
    targetPort: 10259
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true

kubeProxy:
  enabled: true
  endpoints:
    - 10.0.4.101
    - 10.0.4.102
    - 10.0.4.103
  service:
    enabled: true
    port: 10249
    targetPort: 10249

kubeEtcd:
  enabled: true
  # Use selector workaround to auto-populate endpoints from control plane nodes
  # See: https://github.com/siderolabs/talos/discussions/7214
  service:
    enabled: true
    port: 2381
    targetPort: 2381
    selector:
      k8s-app: kube-controller-manager  # Reuse controller-manager selector since all CP nodes run same services
  serviceMonitor:
    enabled: true
    scheme: https
    insecureSkipVerify: true
    # Remove pod label since etcd doesn't run in a pod
    metricRelabelings:
      - action: labeldrop
        regex: pod

grafana:
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: browser
  admin:
    existingSecret: grafana-admin
    userKey: admin-user
    passwordKey: admin-password
  initChownData:
    enabled: false
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
  service:
    type: ClusterIP
  persistence:
    enabled: true
    storageClassName: config-nfs-client
    size: 5Gi
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: Kubernetes
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
  dashboards:
    default:
      kubernetes-cluster-monitoring:
        gnetId: 315
        datasource: Prometheus
      node-exporter-full:
        gnetId: 1860
        datasource: Prometheus
  additionalDataSources:
    - name: Loki
      type: loki
      access: proxy
      url: http://loki-gateway.loki.svc.cluster.local
