crds:
  upgradeJob:
    enabled: false

alertmanager:
  enabled: true
  alertmanagerSpec:
    externalUrl: https://alertmanager.cluster.integratn.tech
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: config-nfs-client
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 5Gi
  config:
    global:
      resolve_timeout: 5m
    inhibit_rules:
    - source_matchers:
      - severity = critical
      target_matchers:
      - severity =~ warning|info
      equal:
      - namespace
      - alertname
    - source_matchers:
      - alertname = InfoInhibitor
      target_matchers:
      - severity = info
    route:
      receiver: matrix
      group_by:
      - namespace
      - alertname
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      routes:
      - receiver: matrix
        matchers:
        - source = argocd-notifications
        group_by:
        - alertname
        - app_name
        group_wait: 10s
        group_interval: 1m
        repeat_interval: 24h
        continue: false
      - receiver: matrix
        matchers:
        - severity = critical
        group_wait: 10s
        repeat_interval: 1h
        continue: false
      - receiver: 'null'
        matchers:
        - alertname = Watchdog
        continue: false
      - receiver: 'null'
        matchers:
        - alertname = InfoInhibitor
        continue: false
    receivers:
    - name: 'null'
    - name: matrix
      webhook_configs:
      - url: http://matrix-alertmanager-receiver.monitoring.svc.cluster.local:12345/alerts/homelab-alerts
        send_resolved: true
prometheus:
  service:
    type: ClusterIP
  prometheusSpec:
    externalUrl: https://prometheus.cluster.integratn.tech
    enableRemoteWriteReceiver: true
    enableFeatures:
    - exemplar-storage
    retention: 15d
    secrets:
    - etcd-client-cert
    serviceMonitorNamespaceSelector: {}
    serviceMonitorSelector: {}
    podMonitorNamespaceSelector: {}
    podMonitorSelector: {}
    ruleNamespaceSelector: {}
    ruleSelector: {}
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: config-nfs-client
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 30Gi
kubeControllerManager:
  enabled: true
  endpoints:
  - 10.0.4.101
  - 10.0.4.102
  - 10.0.4.103
  service:
    enabled: true
    port: 10257
    targetPort: 10257
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true
kubeScheduler:
  enabled: true
  endpoints:
  - 10.0.4.101
  - 10.0.4.102
  - 10.0.4.103
  service:
    enabled: true
    port: 10259
    targetPort: 10259
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true
# kube-proxy is replaced by Cilium eBPF â€” nothing to monitor
kubeProxy:
  enabled: false
kubeEtcd:
  enabled: true
  service:
    enabled: true
    port: 2379
    targetPort: 2379
    selector:
      k8s-app: kube-controller-manager
  serviceMonitor:
    enabled: true
    scheme: https
    insecureSkipVerify: false
    serverName: localhost
    caFile: /etc/prometheus/secrets/etcd-client-cert/ca.crt
    certFile: /etc/prometheus/secrets/etcd-client-cert/server.crt
    keyFile: /etc/prometheus/secrets/etcd-client-cert/server.key
    metricRelabelings:
    - action: labeldrop
      regex: pod
grafana:
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: browser
  sidecar:
    datasources:
      uid: prometheus
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: '1'
      searchNamespace: ALL
      folderAnnotation: grafana_folder
      provider:
        foldersFromFilesStructure: false
  admin:
    existingSecret: grafana-admin
    userKey: admin-user
    passwordKey: admin-password
  # Authentik OAuth2/OIDC configuration
  envValueFrom:
    GF_AUTH_GENERIC_OAUTH_CLIENT_ID:
      secretKeyRef:
        name: grafana-oidc-config
        key: client-id
    GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:
      secretKeyRef:
        name: grafana-oidc-config
        key: client-secret
  grafana.ini:
    server:
      root_url: https://grafana.cluster.integratn.tech
    auth:
      oauth_auto_login: false
      disable_login_form: false
    auth.generic_oauth:
      enabled: true
      name: Authentik
      client_id: $__env{GF_AUTH_GENERIC_OAUTH_CLIENT_ID}
      client_secret: $__env{GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET}
      scopes: openid profile email groups
      auth_url: https://auth.cluster.integratn.tech/application/o/authorize/
      token_url: http://authentik-server.authentik.svc.cluster.local:9000/application/o/token/
      api_url: http://authentik-server.authentik.svc.cluster.local:9000/application/o/userinfo/
      use_pkce: true
      use_refresh_token: true
      allow_sign_up: true
      role_attribute_path: contains(groups[*], 'authentik Admins') && 'Admin' || contains(groups[*], 'authentik Users') && 'Editor' || 'Viewer'
      allowed_groups:
  initChownData:
    enabled: false
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
  service:
    type: ClusterIP
  persistence:
    enabled: true
    storageClassName: config-nfs-client
    size: 5Gi
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: default
        orgId: 1
        folder: Kubernetes
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default
  dashboards:
    default:
      kubernetes-cluster-monitoring:
        gnetId: 315
        datasource: Prometheus
      node-exporter-full:
        gnetId: 1860
        datasource: Prometheus
  additionalDataSources:
  - name: Loki
    type: loki
    uid: loki
    access: proxy
    url: http://loki-gateway.loki.svc.cluster.local
    jsonData:
      maxLines: 5000
      timeout: 60
defaultRules:
  rules:
    etcd: true
  additionalPrometheusRules:
  - name: etcd-override
    groups:
    - name: etcd
      rules:
      - alert: etcdInsufficientMembers
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": insufficient members ({{ $value }}).'
          summary: etcd cluster has insufficient number of members.
        expr: 'sum(up{job=~".*etcd.*"} == bool 1) < ((count(up{job=~".*etcd.*"}) + 1) / 2)

          '
        for: 3m
        labels:
          severity: critical
